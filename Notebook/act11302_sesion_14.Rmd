---
title: "Sesion 14 - Diversificacion via mezclas"
author:
-  Juan Carlos Martínez-Ovando
date: "Otoño 2019"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: yes
    self_contained: yes
    theme: cerulean # cosmo
    highlight: textmate
fig_align: "center"
fig_width: 18

header-includes:
    - \usepackage[most]{tcolorbox}
    - \usepackage{mathtools}
    - \definecolor{light-yellow}{rgb}{1, 0.95, 0.7}
    - \newtcolorbox{myquote}{colback=light-yellow,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
    - \newcommand{\todo}[1]{\begin{myquote} \textbf{TODO:} \emph{#1} \end{myquote}}

---

\newcommand{\WiD}{\operatorname{\text{Wi}}}
\newcommand{\WeD}{\operatorname{\text{We}}}
\newcommand{\WeNormD}{\operatorname{\text{We-N}}}
\newcommand{\ExpD}{\operatorname{\text{Exp}}}
\newcommand{\BeD}{\operatorname{\text{Be}}}
\newcommand{\GeoD}{\operatorname{\text{Geo}}}
\newcommand{\StD}{\operatorname{\text{St}}}
\newcommand{\NormD}{\operatorname{\text{N}}}
\newcommand{\GaD}{\operatorname{\text{Ga}}}
\newcommand{\UniD}{\operatorname{\text{U}}}
\newcommand{\DirD}{\operatorname{\text{Dir}}}
\newcommand{\IG}{\operatorname{\text{InG}}}
\newcommand{\IncGa}{\operatorname{\text{IGa}}}
\newcommand{\IGa}{\operatorname{\text{InGa}}}
\newcommand{\PoD}{\operatorname{\text{Po}}}
\newcommand{\BS}{\operatorname{\text{BS}}}
\newcommand{\DP}{\operatorname{\text{DP}}}
\newcommand{\BinD}{\operatorname{\text{Bin}}}
\newcommand{\BinNegD}{\operatorname{\text{BinNeg}}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\Indic}{\mathbb{I}}
\newcommand{\Borel}{\operatorname{\mathscr{B}}}
\newcommand{\Filtration}{\operatorname{\mathscr{F}}}
\newcommand{\Expec}{\operatorname{\mathbb{E}}}
\newcommand{\Var}{\operatorname{\text{var}}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminares {.tabset .tabset-fade .tabset-pills}

> En la `sesion 13` estudiamos que de manera adicional a la informacion/datos de riesgos (datos acerca de la **frecuencia de siniestros** y de las severidades individuales **severidades individuales**), se puede incluir en el modelo informacion acerca de **variables auxiliares intrinsecas** para **diversidicar el riesgo** de un portafolio de seguros.

Asi, si las **variables auxiliares** son de tipo categoricas, de manera que una o combinacion de varias variables adicionales inducen la **segmentacion** del protafolio en $K$ categorias, entonces el portafolio de seguros puede modelarse via **modelos jerarquicos** con $K$-segmentos, i.e.
\begin{eqnarray}
Y_{ik}|k,\theta_k & \sim & F_k(y|\theta_k) \\
\theta_k & \sim & G(\theta|\gamma),
\end{eqnarray}
para $k=1,\ldots,K$ e $i=1,\ldots,I_k$, donde

* $F_k(y|\theta_k)$ es un modelo de probabilidad para el segmento de riesgo $k$, parametrizado por $\theta_k$ (esta distribucion mide la incertidumbre al interior del segmento)

* $G(\theta|\gamma)$ es un modelo de probabilidad para los parametros de todos los segmentos (esta distribucion mide la incertidumbre entre segmentos), a su vez parametrizada por $\gamma$ (parametro comun a todos los segmentos)

* se supone que $\theta_k\neq\theta_l$ para todo $k\neq l$ (para que la especificacion del modelo tenga sentido).

Adicionalmente, se supone que 
$$
\gamma \sim \Pi(\gamma),
$$
cuantifica la informaion suplemental acerca de $\gamma$.

De manera que para todo $i$ y $k$ se tiene que el **modelo marginal**,
$$
P(y|\gamma) = \int F_k(y|\theta_k)G(d \theta_k|\gamma),
$$
es el mismo (por lo que en el agregado, el supuesto de `homogeniedad` se cumple).

> Recodemos que para que esto tenga sentido, la informacion de las variables auxiliares debe inducir que cada poliza pertenezca a uno y solo uno de los segmentos de riesgos distintos.

## Observacion

> En muchas ocasiones, podemos sospechar que al interior del portafolio subyacen diferentes exposiciones al riesgo, pero **no es posible identificar** para una poliza particular a que riesgo corresponde. 

En ese caso, la enumeracion de los $K$ segmentos de riesgo no existe, y la asignacion del tipo de riesgo a cada poliza tampoco es posible de realizar sin error.

En este caso, se recomienda manigestar en el modelo la posibilidad de asociar diferentes segmentos de riesgo para cada poliza empleando **mezclas de modelos**. 

Las mezclas de modelos, quedan especificadas como
$$
Y_{i} \sim F(y|\gamma),
$$
donde 
$$
F(y|\gamma) = \sum_{l=1}^{L}w_l F_l(y|\theta_l),
$$
con

* $F_l(y|\theta_l)$ cuantifica la incertidumbre al interior del **segmento de riesgo no revelado** $l$,

* $(w_1,\ldots,w_L)$ son valores en el simplejo de dimension $(L-1)$, y cuantifican la propension a que una poliza sea asociada con cada uno de los $L$ **segmentos se riesgo no revelados**.

Este modelo puede expresearse _jerarquicamente_ incorporando una **variable de asginacion de segmento** $\iota$ a cada poliza, i.e.
\begin{eqnarray}
Y_i|\iota_i=l &\sim & F_l(y|\theta_l) \\
\iota_i & \sim & \text{Mult}(\iota|w_1,\ldots,w_L),
\end{eqnarray}
para $i=1,\ldots,I$.

> Cuando $L$ es **finito** se dice que se trabaja con modelos de **mezclas finitas**.

## Estimacion

La estimacion de **modelos de mezclas finitas** puede realizase de la siguiente forma:

a. Caso frecuentista, via el `algoritmo EM` (Expectation-Maximization)

b. Caso bayesiano, via el `algoritmo Gibbs sampler`.

> en esta sesion haremos uso del `algoritmo EM`.

## Caso general

> Mas aun, podemos pensar que cada uno de los riesgos asociados con las polizas, $Y_i$ para $i=1,\ldots,I$, puede esyar acompaniado con **informacion adicional revelada**, denotada por $z_i$ (puede ser multidimensional, categorica o numerica escalar), de manera que el modelo de probabilidad para $Y_i$ puede ser **dependiente en z_i**, de manera que 
$$
Y_{i}|z_i \sim F(y|z_i,\gamma),
$$
donde 
$$
F(y|z_i,\gamma) = \sum_{l=1}^{L}w_l F_l(y|z_i,\theta_l),
$$
con

* $F_l(y|z_i,\theta_l)$ cuantifica la incertidumbre al interior del **segmento de riesgo no revelado** $l$, indizada por $z_i$ (como un modelo de regresion, por ejemplo)

* $(w_1,\ldots,w_L)$ son valores en el simplejo de dimension $(L-1)$, y cuantifican la propension a que una poliza sea asociada con cada uno de los $L$ **segmentos se riesgo no revelados**.

> Estos modelos se conocen como **modelos de mezclas finitas de regresion**.

A continuacion veremos como implementar estos modelos en diferentes contextos.

# Mezclas de regresion binomial  {.tabset .tabset-fade .tabset-pills}

En este caso consideramos que la variable de interes, $Y_i$, es discreta en la forma de conteo o dicotomica, i.e.
$$
Y_i|\iota_i=l \sim \text{Bin}(y|J,\theta_{il}),
$$
donde 
\begin{eqnarray}
\theta_{il} & = & \mathbb{P}(\text{siniestro})  \\
  & = & g(\beta_l'z_i),
\end{eqnarray}
siendo $\beta_l$ un vector de coeficientes de regresion para el **segmento no evidente** $l$, y $z_i$ un conjunto de covariables.

Por otro lado, 
$$
\mathbb{P}\left(\iota_{i} = l\right) = w_l,
$$
para todo $i$.

> Marginalizando $\iota_i$ tenemos
$$
Y_i \sim \sum_{l=1}^L w_l \text{Bin}(y|J,\theta_{il}).
$$

## Implementacion

Veamos como funciona este modelo en la practica. Los datos para implementar este tipo de modelos deben estar estructurados de la siguiente forma.

```{r}
if(!require("flexmix")){install.packages("flexmix")}
library("flexmix")

data("NregFix", package = "flexmix")

dim(NregFix)

tail(NregFix)

colnames(NregFix)
```
Con la siguiente instruccion indicaremos que **covariables** del `data.frame` `NregFix` consideraremos en el modelo.

```{r}
Model <- FLXMRglm(~ x2 + x1)
Model
```

La implementacion computacional del modelo via el `algoritmo EM` (i.e. via simulacion estocastica), se realiza de la siguiente forma:

```{r}
set.seed(12345)

fittedModel <- stepFlexmix(y ~ 1, model = Model, 
                           nrep = 3, k = 3,
                           data = NregFix, concomitant = FLXPmultinom(~ w))

fittedModel

fittedModel <- relabel(fittedModel, "model", "x1")

fittedModel
```

* En este caso, consideramos `k = 3` posibles clases. 

* El modelo considera la inclusion de una ordenada al origen en el componente de regresion, por eso la especificacion `y ~ 1`.

* Con la funcion `relabel` se etiquetan de nuevo los grupos encontrados en la mezcla. Esto es por una consideracion de identificabilidad (se conoce a este tema como *label-switching problem*).

> El resumen de esta implementacion es como sigue:

```{r}
summary(refit(fittedModel))
```

* Una `estimacion ingenua` de las probabilidades $(w_1,w_2,w_3)$ se obtendria a partir de los `Cluster sizes`, i.e
$$
\hat{w}_1 = 95 / 200, \\
\hat{w}_2 = 74 / 200, \\
\hat{w}_3 = 31 / 200. \\
$$

## Modelo complementario

Consideremos ahora un modelo complementario a `Model`. Este es etiquetado como `Model2`, el cual incluye ordenada al origen tambien (i.e. `y ~ 1`).

```{r}
Model2 <- FLXMRglmfix(fixed = ~ x2, 
                      nested = list(k = c(1, 2),
                      formula = c(~ 0, ~ x1)), varFix = TRUE)

fittedModel2 <- flexmix(y ~ 1, model = Model2,
                        cluster = posterior(fittedModel), 
                        data = NregFix,
                        concomitant = FLXPmultinom(~ w))
fittedModel2
```

> Noten que la composicion de los grupos en `Model2` es marginalmente distinta.

El resumen de `Model2` es el siguiente:

```{r}
summary(refit(fittedModel2))
```

> Observen que la especificacion de `Model2` permite identificar un conjunto de covariables potencialmente distinto al interior de los grupos (caso `Comp1`).

## Comparacion de modelos

Habiendo implementado dos modelos que potencialmente resultan significativos y cuyas diferencias son sutiles y relevantes, la pregunta ahora reside en:

> Como elegir el **mejor modelo** entre estas alternativas.

Como vimos, criterior de bondad de ajuste en este caso no aplicarian, porque `Model` y `Model2` son estructuralmente distintos, siendo ambos bastante flexibles.

> El criterio que elegimos es basado en la **metrica de devianza**:

```{r}
BIC(fittedModel)

BIC(fittedModel2)
```

> Que modelo elegiriamos???

# Mezcla de regresion Gaussiana

En este caso, la variable de inetres es $Y_i$ con soporte en la recta real, y el modelo seria
$$
Y_i \sim \sum_{l=1}^L w_l \text{N}(y|\mu_{il},\sigma^2_l),
$$
donde 
$$
\mu_{il} = \beta_l'z_i.
$$
* Este es el caso de mezclar $L$ modelos de regresion convencionales.

> Noten que el modelo es util para describir la variabilidad subyacente a las **severidades individuales** a traves de la transformacion de los montos asociada con la distribucion `logNormal`.

## Implementacion

De igual forma, los datos deben estar estructurados como un `data.frame`.


```{r}
data("NPreg", package = "flexmix")

dim(NPreg)

colnames(NPreg)

tail(NPreg)
```

Ahora, implementamos la mezcla de dos componetes, i.e. $L=2$.

```{r}
Model.G <- flexmix(yn ~ x + I(x^2), 
               data = NPreg, k = 2,
               control = list(verb = 5, iter = 100))

Model.G

summary(Model.G)
summary(refit(Model.G))

plot(Model.G)
```

> Noten que `Comp2` no es significativo.

## Modelo complementario

Al igual que en la ilustracion anterior, consideraremos ahora un modelo complementario **bastante loco** (por motivos ilustrativos), en el que:

a. Un componente es Gaussiano

b. Otro componente es Poisson.

```{r}
Model3 <- flexmix(yn ~ x, data = NPreg, k = 2,
               model = list(FLXMRglm(yn ~ . + I(x^2)),
                            FLXMRglm(yp ~ ., family = "poisson")))
plot(Model3)
Model3
table(Model3@cluster, NPreg$class)
```

> Los parametros asociados con el componente Gaussiano:

```{r}
parameters(Model3, component = 1, model = 1)
```

> Los parametros asociados con el componente Poisson:

```{r}
parameters(Model3, component = 1, model = 2)
```

```
## fitting a model only to the Poisson response is of course
## done like this
ex3 <- flexmix(yp ~ x, data = NPreg, k = 2,
model = FLXMRglm(family = "poisson"))
## if observations are grouped, i.e., we have several observations per
## individual, fitting is usually much faster:
ex4 <- flexmix(yp~x|id1, data = NPreg, k = 2,
model = FLXMRglm(family = "poisson"))## And now a binomial example. Mixtures of binomials are not genericall## identified, here the grouping variable is necessary:set.seed(1234)ex5 <- initFlexmix(cbind(yb,1 - yb) ~ x, data = NPreg, k = 2,model = FLXMRglm(family = "binomial"), nrep = 5)table(NPreg$class, clusters(ex5))ex6 <- initFlexmix(cbind(yb, 1 - yb) ~ x | id2, data = NPreg, k = 2,model = FLXMRglm(family = "binomial"), nrep = 5)table(NPreg$class, clusters(ex6
```

# Mezclas de regresion Poisson  {.tabset .tabset-fade .tabset-pills}

Consideraremos ahora el caso en que la **variable de interes** es $Y_i$ con soporte en los enteros positivos, y consideramos que el modelo es de la forma
$$
Y_i \sim \sum_{l=1}^L w_l \text{Po}(y|\lambda_{il}),
$$
donde
$$
\lambda_{il} = \exp\left\{ \beta_l ' z_i\right\}.
$$

## Implementacion

Al igual que antes, los datos deben estar estructurados como un `data.frame`:

```{r}
data("fabricfault", package = "flexmix")

dim(fabricfault)

colnames(fabricfault)

tail(fabricfault)
```

```{r}
Model4 <- stepFlexmix(Faults ~ 1, model = FLXMRglmfix(family="poisson",
                                                         fixed = ~ log(Length)), data = fabricfault, k = 2, nrep = 3)
summary(Model4)

summary(refit(Model4))
```

## Prediccion

> Recordemos que el proposito de estos modelos, con relacion a nuestro curso, es el de producir **predicciones** sobre valores futuros.

En el contexto de regresion, la prediccion se realiza de manera *mas o menos controlada*, considerando que a partir de **covariables**,
$$
z_1^f,\ldots,z_m^f,
$$
podemos general --a partir del modelo--, los **valores predictivos**
$$
Y_1^f,\ldots,Y_m^f.
$$
La generacion de valores predictivos, en el caso de mezclas, se obtiene facilmente de la siguiente forma:

```{r}
Znew <- seq(0, 1000, by = 50)

fabricMix.pred <- predict(Model4, newdata = data.frame(Length = Znew))

head(fabricMix.pred)
```

### Considerando el modelo complementario

```{r}
fabricMix2 <- flexmix(Faults ~ 0, 
                      data = fabricfault,
                      cluster = posterior(Model4),
                      model = FLXMRglmfix(family = "poisson", fixed = ~ log(Length),
                                          nested = list(k=c(1,1), formula=list(~0,~1))))

summary(refit(fabricMix2))

fabricMix2.pred <-  fabricMix2.pred <- predict(fabricMix2,
                                               newdata = data.frame(Length = Znew))
head(fabricMix2.pred)
```

> Noten que las predicciones pueden ser significativamente distintas, en algunos casos. 

> Por eso, necesitamos ser cuidadosos con la especificacion y eleccion del modelo para prediccion.

# Lecturas complementarias  {.tabset .tabset-fade .tabset-pills}

* Frühwirth-Schnatter, Sylvia, *Finite Mixture and Markov Switching Models*, Springer 2006.

* Ferrari & Cribari-Neto. *Beta Regression for Modeling Rates and Proportions.* Journal of Applied Statistics, 2004.

* Simas et al. *Improved Estimators for a General Class of Beta Regression Models.* Computational Statistics & Data Analysis, 2010.