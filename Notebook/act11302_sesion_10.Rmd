---
title: "Sesion 10 - Estimacion de Distribuciones / Contruccion de Modelos"
author:
-  Juan Carlos Martínez-Ovando
date: "Otoño 2019"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: yes
    self_contained: yes
    theme: cerulean # cosmo
    highlight: textmate
fig_align: "center"
fig_width: 18

header-includes:
    - \usepackage[most]{tcolorbox}
    - \usepackage{mathtools}
    - \definecolor{light-yellow}{rgb}{1, 0.95, 0.7}
    - \newtcolorbox{myquote}{colback=light-yellow,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
    - \newcommand{\todo}[1]{\begin{myquote} \textbf{TODO:} \emph{#1} \end{myquote}}

---

\newcommand{\WiD}{\operatorname{\text{Wi}}}
\newcommand{\WeD}{\operatorname{\text{We}}}
\newcommand{\WeNormD}{\operatorname{\text{We-N}}}
\newcommand{\ExpD}{\operatorname{\text{Exp}}}
\newcommand{\BeD}{\operatorname{\text{Be}}}
\newcommand{\GeoD}{\operatorname{\text{Geo}}}
\newcommand{\StD}{\operatorname{\text{St}}}
\newcommand{\logNormD}{\operatorname{\text{logN}}}
\newcommand{\NormD}{\operatorname{\text{N}}}
\newcommand{\GaD}{\operatorname{\text{Ga}}}
\newcommand{\UniD}{\operatorname{\text{U}}}
\newcommand{\DirD}{\operatorname{\text{Dir}}}
\newcommand{\IG}{\operatorname{\text{InG}}}
\newcommand{\IncGa}{\operatorname{\text{IGa}}}
\newcommand{\IGa}{\operatorname{\text{InGa}}}
\newcommand{\PoD}{\operatorname{\text{Po}}}
\newcommand{\BS}{\operatorname{\text{BS}}}
\newcommand{\DP}{\operatorname{\text{DP}}}
\newcommand{\BinD}{\operatorname{\text{Bin}}}
\newcommand{\BinNegD}{\operatorname{\text{BinNeg}}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\Indic}{\mathbb{I}}
\newcommand{\Borel}{\operatorname{\mathscr{B}}}
\newcommand{\Filtration}{\operatorname{\mathscr{F}}}
\newcommand{\Expec}{\operatorname{\mathbb{E}}}
\newcommand{\Var}{\operatorname{\text{var}}}

---

# Objetivos

* Estudiaremos la especificacion de diferentes modelos de probabilidad para modelar las severidades individuales.

* Revisaremos diferentes procedimientos para comparar y seleccionar modelos de probabilidad en este contexto.

* Ilustraremos esta sesion con los datos de `AllState`.

---


# Modelo de severidades individuales {.tabset .tabset-fade .tabset-pills}

Consideremos, como en ocasiones previas, los datos de `AllState`:

```{r}
data <- read.csv("./Datos/d4a_allstateclaim_data.csv")
head(data)
colnames(data)
```

Recordemos, los datos corresponden a reclamos por polizas de un seguro de autos de la empresa _AllState_. Se supone que cada poliza es contratada por un hogar. Los montos de reclamos para las diferentes polizas pueden diferenciarse por anio de suscripcion, modelo del vehiculo y fabricante.

## Diccionario

- `Household_ID` INT - Identificador numerico de la poliza (una por hogar).

- `Vehicle` INT - Identificador del vehiculo asegurado por poliza (dentro de cada hogar).

- `Calendar_Year` INT - Anio calendario en el que el vehiculo fue asegurado.

- `Model_Year` INT - Anio calendario de fabricacion/venta del vehiculo. 

- `Blind_Make` INT - Fabricante del vehiculo (discresional).

- `Claim_Amount` INT - Monto de reclamo asociado con el vehiculo (montos en USD).

**Nota:-** Las polizas con reclamos nulos (`Claim_Amount`==0) corresponden a casos no siniestrados.

## Datos de severidades individuales

Los datos que son de interes para nosotros, en este laboratorio, son los de `severidades individuales` condicionales en la `ocurrencia de siniestros`.

Estos datos corresponden a los casos donde `Claim_Amount`>0). Para esto, filtramos los datos para recuperar las severidades individuales.

```{r}
indices <- which(data$Claim_Amount > 0)
severidades <- data[indices,]
dim(severidades)
table(severidades$Calendar_Year)
```

Conservamos todas las variables, porque realizaremos un ejercicio de diversificacion mas adelante. En particular, referente a el *anio calendario* (`Calendar_Year`) y al fabricante del vehiculo (`Blind_Make`).

```{r}
table(severidades[,c("Calendar_Year","Blind_Make")])
```

De momento, en este laboratorio, solo consideraremos los datos `Claim_Amount`.

**Nota:-** Recordemos que los datos de `montos/severidades individuales` son reportados en la misma base monetaria, por lo que estos son comparables entre los casos `2005`, `2006` y `2007`.

Observemos la comparacion de los datos entre anios:

```{r}
if(!require("ggplot2")){install.packages("ggplot2")}
library("ggplot2")

severidades$Calendar_Year <- as.factor(severidades$Calendar_Year)

ggplot(severidades[,c("Calendar_Year","Claim_Amount")], aes(x=Claim_Amount,group=Calendar_Year,fill=Calendar_Year))+
  geom_histogram(position="dodge",binwidth=0.25)+theme_bw()
```

Podemos focalizar tal comparacion, eliminando los datos `Claim_Amount`<1000:

```{r}
ggplot(severidades[which(severidades$Claim_Amount<1000),c("Calendar_Year","Claim_Amount")], aes(x=Claim_Amount,group=Calendar_Year,fill=Calendar_Year))+
  geom_histogram(position="dodge",binwidth=0.25)+theme_bw()
```

*Grosso modo*, los datos comparten el mismo comportamiento:

**2005**

```{r}
summary(severidades[which(severidades$Calendar_Year=="2005"),"Claim_Amount"])
```

**2006**

```{r}
summary(severidades[which(severidades$Calendar_Year=="2006"),"Claim_Amount"])
```

**2007**

```{r}
summary(severidades[which(severidades$Calendar_Year=="2007"),"Claim_Amount"])
```

Observamos un patron marginalmente diferenciado de los datos de `2005` y `2006` respecto a `2007` en terminos de los siguientes aspectos de su distribucion asociada:

* **Sesgo**

* **Peso de la cola derecha**

Examinaremos estos aspectos con detalle mas adelante. Por lo pronto, consideremos los siguientes escenarios de `entrenamiento` y `prueba`.

# Escenarios de aprendizaje


## Aprendizaje I

Datos de `entrenamiento` para `2005` y de `prueba` para `2006`. 

## Aprendizaje II

Datos de `entrenamiento` para `2005,2006` y de `prueba` para `2007`. 

# Definiciones y propiedades 

## Preliminares

* El estudio de montos de siniestralidad (individual y/o agregada) requiere entender la posibilidad de incidencia de riesgos de montos grandes. 

* Para eso, es escencial entender y cuantificar observaciones *ralas* caracterizadas por el **extremo derecho de la probabilidad subyacente** (a.k.a. *right-tail*). 

> Tal probabilidad se define en terminos de la incidencia condicional como la **funcion hazard**, $h(x)$, definida como
$$
h_{X}(x):=\frac{f_{X}(x)}{1-F_{X}(x)}\mathbb{I}_{[0,\infty)}(x),
$$
donde $f_{X}(x)$ denota la *densidad de probabilidades*, y $F_{X}(x)$ la *distribucion de probabilidades acumuladas*.

De esta definicion se desprende la **funcion de severidades esperadas en exceso** de una cierta cantidad $x^*$, la cual se define como
\begin{eqnarray}
  e_X(x^*) & = & \mathbb{E}_{F_X}(X|X>x^*) - x^* \nonumber \\
         & = &  \frac{\int_{x^*}^{\infty}(x-x^*)f_{X}(x)dx}{\int_{x^*}^{\infty}xf_{X}(x)dx},\nonumber
\end{eqnarray}
definida para todo $x^* \geq 0$.

Las funciones $h(x)$ y $e(x)$ estan mutuamente relacionadas, en el caso continuo, como:
$$
h_X(x) = \frac{1+e_X'(x)}{e_X(x)},
$$
para $x\geq 0$.

De estas definiciones se desprenden algunas consideraciones adicionales:

a. La funcion $h_X(x)$ caracteriza el cambio de probabilidad de incidencia para motos en exceso de $x$, y como tal probabilidad decrece cuando $x$ se incrementa.


b. El comportamiento anterior da origen a la nocion de **variacion regular**, en la cual se define la tasa de cambio en $h(\cdot)$ antes variaciones escalares de $x$, i.e.
$$
\lim_{x\rightarrow \infty} \frac{h_{X}(tx)}{h_{X}(x)}=t^\rho,
$$
donde $\rho$ denota el indice de la variacion regular de la funcion $h_X(\cdot)$. 

> Asi, tenemos los siguientes casos:
1. caso $\rho = 0$, refiere a *variaciones lentas a infinito*.
2. caso $\rho \neq 0$, refiere a *variaciones regulares a infinito*.

> De esta forma, los **modelos de probabilidad** para severidades individuales son tipicamente estudiados en terminos de la *funcion hazard*. Comportamientos semejantes de diferentes funciones $h_X(x)$ definen **sistemas de distribuciones de probabilidad** (revisen la seccion 2.3 de Kleiber y Kotz para mas detalles).

## Modelos

Recordemos, en este laboratorio solo estamos prestando atencion al estudio de la `distribucion de severidades individuales`. De esta forma, y de manera consistente con las distribuciones de la `session 22` de nuestro curso, porponemos los siguientes modelos:

### Funcion de distribucion empirica (EDF, por sus siglas en ingles)

Obtendremos la estimacion de la EDF empleando la libreria `EnvStats`. Recuerden, que el calculo es relativamente sencillo de obtener sin emplear esta libreria.

### Distribucion Pareto


La dsitribucion Pareto se define como
$$
F_X(x|\tau,\alpha)=\left(1-\left(\frac{x}{\tau}\right)^{-\alpha}\right)\mathbb{I}_{[\tau,\infty)}(x),
$$
para $\tau>0$ y $\alpha>0$. De esta forma, la funcion de densidad asociada es
$$
f_X(x)=\frac{\alpha\tau^{\alpha}}{x^{\alpha+1}}\mathbb{I}_{[\tau,\infty)}(x).
$$

* Cuando $\alpha$, el parametro de forma, es pequeno el peso de la *cola derecha de la distribucion* es mayor. 

* Los momentos de la distribucion Pareto tienen una forma analitica cerrada, dada por
$$
\mathbb{E}\left(X^k\right) = \frac{\alpha \tau^k}{\alpha-k}.
$$

> Estimar los parametros $\alpha$ (forma) y $\tau$ (escala), de manera conjunta, no tiene una solucion analitica cerrada. Usualmente se emplean metodos numericos, sobre todo para corregir por sesgos de los estimadores, en particular asociados con el parametro de escala, $\tau$, que restringe el soporte de la distribucion.

La estimacion de parametros via `maxima verosimilitud` y `minimos cuadrados` puede obtenerse en `R` empleando la libreria `EnvStats`.

### Distribucion Lognormal

La distribucion lognormal se define como
$$
F_X(x|\mu,\sigma)=\Phi\left(\frac{\log x - \mu}{\sigma}\right)\mathbb{I}_{(0,\infty)}(x),
$$
donde $\Phi(\cdot)$ denota la funcion de distribucion Gaussiana estandar, siendo $\exp\{\mu\}$ el parametro de escala y $\sigma$ el parametro de forma.

La funcion de densidad asociada es de la forma,
$$
f_{X}(x)=\frac{1}{x\sigma(2\pi)^{1/2}}\exp\left\{-\frac{1}{2\sigma^2}(\log x-\mu)^2\right\}\mathbb{I}_{(0,\infty)}(x).
$$
* Este modelo no tiene una *funcion hazard* de manera analitica cerrada.

* Los momentos de la distribucion lognormal pueden recuperarse como
$$
\mathbb{E}\left(X^k\right) = \exp\left(k\mu + \frac{(k\sigma)^2}{2}\right).
$$


> Encontrar estimadores puntuales para $\mu$ y $\sigma$ es relativamente simple, debido a la conexion de la distribucion lognormal con la distribucion gaussiana. Sin embargo, las propiedades de los estimadores en el caso gaussiano no necesariamente se heredaran al caso lognormal. Revisen la referencia de Keliber & Kotz.

En `R`, la estimacion de parametros via `maxima verosimilitud` puede obtenerse empleando la libreria `MASS`.

### Distribucion gamma generalizada

La distribucion gamma generalizada se define a partir de la funcion de densidad de probabilidades,
$$
f_X(x|\alpha,\beta,\tau) = \frac{\alpha}{\beta^{\alpha\tau}\Gamma(\tau)}x^{\alpha\tau-1}\exp\left\{-\left(\frac{x}{\beta}\right)^\alpha\right\}\mathbb{I}_{(0,\infty)}(x),
$$
donde $\beta>0$ es el parametro de escala y $(\alpha,\tau)$ son parametros de forma.

* Esta distribucion no tiene una forma analitica cerrada para su funcion de distribucion.

* Esta distribucion incluye como caso particular a la distribucion Weibull, cuando $\tau=1$ y $\alpha>0$.

* Los momentos de esta distribucion pueden recuperarse como
$$
\mathbb{E}\left(X^k\right) = \frac{\beta^k\Gamma(\tau + k/\alpha)}{\Gamma(\tau)}.
$$

> La estimacion de parametros en este tipo de distirbuciones involucra el doble reto de estimar $(\alpha,\tau)$, parametros de forma, conjuntamente. Esto es porque combinaciones de valores distintos de estos dos parametros pueden generar distribuciones que son muy parecidas en casi todo el soporte de $X$, causando una pseudo falta de identificabilidad estructural del modelo.

La estimacion de parametros via `maxima verosimilitud` puede obtenerse en `R` (con precaucion) empleando la libreria `VGAM`.

### Distribucion beta generalizada del segundo tipo

La distribucion beta generalizada del segundo tipo de define como la modificacion de la distribuciones beta dada por,
$$
F_X(x|\tau_1,\tau_2,\alpha,\beta) = I_z(\tau_1,\tau_2)\mathbb{I}_{(0,\infty)}(x),
$$
donde 
$$
z:=\left(\frac{x}{\beta}\right)^\alpha,
$$
y
$$
I_z(\tau_1,\tau_2) := \frac{1}{B(\tau_1,\tau_2)}\int_{0}^{z}\frac{s^{\tau_1 - 1}}{(1+s)^{\tau_1+\tau_2}}ds.
$$

Derivado de la expresion anterior, la densidad asociada es de la forma
$$
f_X(s) = \frac{\alpha x^{\alpha\tau_1 - 1}}{\beta^{\alpha\tau_1} B(\tau_1,\tau_2)\left(1+\left(\frac{x}{\beta}\right)^\alpha\right)^{\tau_1+\tau_2}}\mathbb{I}_{(0,\infty)}(x).
$$

* Siendo, $\beta$ el parametro de escala, y $(\alpha,\tau_1,\tau_2)$ parametros de forma (todos positivos). Los ultimos tres parametros controlan la forma de la cola derecha de la distirbucion, definiendo variacion regular en combinaciones de $(\alpha,\tau_1)$ y $(\alpha,\tau_2)$.

* Com consecuencia del punto anterior, los momentos para esta distribucion existiran en el caso en que 
$$
-\alpha\tau_1 < k < \alpha\tau,
$$
y estan dados por
$$
\mathbb{E}\left(X^k\right) = \beta^k \frac{B\left(\tau_1+k/\alpha,\tau_2-k/\alpha\right)}{B(\tau_1,\tau_2)}.
$$

> La estimacion puntual de los cuatro parametros de esta distirbucion es bastante complicada, y descansa en la actualidad en el uso de metodos numericos de optimizacion sobre una funcion de verosimilitud con forma analitica cerrada. Las propiedades teoricas de estos estimadores son materia de estudio en la actualidad.

En `R`, la estimacion de los parametros de la distribucion beta generalizada del segundo tipo via `maxima varosimilitud` pueden obtenerse empleando la libreria `GB2`.

### Distribucion Pareto generalizada

La distribucion Pareto generalizada se obtiene como la distribucion asintotica de valores en exceso de un cierto umbral (*Peak-Over-Threshold*). 

La funcion de supervivencia (cola derecha de la distribucion), se define como
$$
\bar{F}_{X}(x) = 
\begin{cases}
\left(1+\frac{\alpha x}{\beta}\right)^{-1/\beta}, & \text{ si } \alpha\neq 0, \\
\exp\left\{-\frac{x}{\beta}\right\},&\text{ si }\alpha=0,
\end{cases}
$$
con soporte en 
$$
\mathcal{X}_{\alpha,\beta} = 
\begin{cases}
[0,\infty), & \text{ si }\alpha\geq 0, \\
[0,-\beta/\alpha),& \text{ si }\alpha < 0.
\end{cases}
$$

> La estimacion de parametros via maxima verosimilitud require el uso de metodos de optimizacion numerica, aunado a un tratamiento cuidadoso del dominio de restriccion/soporte involucrado.

En `R` la estimacion de los parametros via `maxima verosimilitud` de esta distribucion puede obtenerse empleando la libreria `evir`. Esta libreria incluye otros metodos de estimacion, de manera alternativa, pero no los consideraremos en este curso.


```{r include=FALSE}
if(!require("EnvStats")){install.packages("EnvStats")}
if(!require("VGAM")){install.packages("VGAM")}
if(!require("MASS")){install.packages("MASS")}
if(!require("GB2")){install.packages("GB2")}
if(!require("evir")){install.packages("evir")}
```

## Racionalidad

> Hemos revisado cinco distribuciones de probabilidad cuyo proposito es el de flexibilizar la especificacion de la funcion de distribucion de severidades individuales (por el momento). La EDF flexibiliza la asignacion de probabilidades en la region de alta concentracion de datos, mientras que las otras alternativas parametricas flexibilizan la asignacion de probabilidades en el extremo derecho.

> Todas estas expecificaciones son potencialmente factibles. Sin embargo, por consideraciones practicas, es requerido definir procedimientos de **comparacion** de estas distribuciones y criterior para **seleccionar la mejor alternativa** a empleas. Mas adelante revisaremos este topico.


# Proceso de aprendizaje I


En el `Aprendizaje I` consideramos los datos de `2005` como `datos de entrenamiento` y los de `2006` como `datos de prueba`.

```{r}
severidades.train <- severidades[which(severidades$Calendar_Year=="2005"),"Claim_Amount"]
severidades.test <- severidades[which(severidades$Calendar_Year=="2006"),"Claim_Amount"]
hist(severidades.train,50)
```

## Funcion de distribucion empirica

La estimacion de la $edf$ se obtiene con la siguiente instruccion:
```{r}
library("EnvStats")
p <- qemp(p = seq(0, 1, len = 100), severidades.train) 
f.edf <- demp(p, severidades.train) 
```

La visualizacion de la $edf$ en los datos se obtiene como:
```{r}
epdfPlot(severidades.train, 
         xlim = c(0, 65), 
         epdf.col = "cyan", 
         xlab = "X", 
         main = "EDF")
epdfPlot(severidades.train, 
         xlim = c(0, 2000), 
         epdf.col = "cyan", 
         xlab = "X", 
         main = "EDF")
```

## Distribucion Pareto

## Distribucion Lognormal

La estimacion de la distirbucion log-normal se obtiene de la siguiente forma:
```{r}
library("MASS")
par.lognorm <- fitdistr(severidades.train,"lognormal")
par.lognorm
```

La densidad de la distribucion estimada es:
```{r}
severidades.train.unique <- sort(unique(severidades.train))
d.lognorm <- dlnorm(severidades.train.unique, 
                    par.lognorm$estimate['meanlog'], 
                    par.lognorm$estimate['sdlog'])

plot(severidades.train.unique, d.lognorm, type="l", 
     ylab="Densidad",
     xlab="X", 
     xlim=c(0,65))
title(main = "Densidad lognormal")

plot(severidades.train.unique, d.lognorm, type="l", 
     ylab="Densidad",
     xlab="X", 
     xlim=c(0,2000))
title(main = "Densidad lognormal")
```

## Distribucion gamma generalizada

## Distribucion beta generalizada del segundo tipo

La estimacion de los parametros de la distribucion beta generalizada del segundo tipo se obtene de la siguiente forma:
```{r}
library("GB2")
par.gb2 <- mlfit.gb2(severidades.train)
par.gb2
```

## Distribucion Pareto generalizada

# Comparacion de modelos

En esta secion implementaremos el procedimiento de `comparacion` y `seleccion` del modelo para *severidades individuales*. Realizaremos la ilustracion con tres enfoques:

a. Bondad de ajuste

b. Cociente de verosimilitud

c. Cociente de veroimilitud extendida

## Bondad de ajuste

Este enfoque basicamente consiste en contrastar los **cinco modelos parametricos** en terminos de sus `cercania/semejanza` con la **EDF**.

Los criterios difieren, escencialmente, en la `metrica` empleada para calcular la `cercania/semejanza` entre distribuciones.

Definimos, de manera general, a $F_{edf}(X)$ como la funcion de distirbucion empirica y $F_j(x|\hat{\theta}_j)$ como la distirbucion parametrica estimada (puntualmente) del tipo de modelo $j$.

### Kolmogorov-Smirnov

El estadistico para medir la `cercania/semejanza` entre distribuciones $edf$ y $j$ bajo Kolmogorov-Smirnov (KS) es
$$
T_{KS}(j)=\max_{x}|F_{edf}(x)-F_j(x|\hat{\theta}_j)|,
$$
para $j=1,\ldots,J$.

**Nota.** La $edf$ es discreta, por lo que el calculo de $T_{KS}(j)$ debe realizarse en dos puntos para cada valor de $x$:

* $F_{edf}(x-)$ que es el limite por la izquierda antes de $x$

* $F_{edf}(x)$ que es el limite por la derecha antes de $x$.

Los valores criticos tradicionalmente empleados para esta estadistica son:

1. $T_\alpha=\frac{1.22}{\sqrt{n}}$ para $\alpha=0.10$ nivel de significacia,

2. $T_\alpha=\frac{1.36}{\sqrt{n}}$ para $\alpha=0.05$ nivel de significacia,

3. $T_\alpha=\frac{1.63}{\sqrt{n}}$ para $\alpha=0.01$ nivel de significacia,

donde $n$ es el tamnio de la muestra.

### Anderson-Darling

En la opcion anderson-Darling (AD), la metrica para medir la `cercania/semejanza` entre distribuciones $edf$ y $j$ es definida como
$$
T_{AD}(j)=-nF_j(x^*_U|\hat{\theta}_j)
+n\sum_{u=1}^{U-1} [1-F_{edf}(x^*_u)]^2 \left\{\text{log}[1-F_j(x^*_u|\hat{\theta}_j)]-\text{log}[1-F_j(x^*_{u+1}|\hat{\theta}_j)]\right\}
+ n \sum_{u=2}^{U} F_{edf}(x_u^*)^2 \left[\text{log}F_j(x^*_{u+1}|\hat{\theta}_j)-\text{log}F_j(x^*_u|\hat{\theta}_j)\right],
$$
donde $\{x_u^*\}_{u=1}^{U}$ corresponden a los $U$ datos unicos disponibles.

**Nota:** En este calculo, se hace explicito la consideracion de comparacion entre `limites por la derecha` y `limites por la izquierda` relacionados con $F_{edf}(\cdot)$.

Los valores critios, para esta prueba, son tipicamente:

1. $T_a=1.933$ para $a=0.10$,

2. $T_a=2.492$ para $a=0.05$,

3. $T_a=3.857$ para $a=0.01$.

## Verosimilitud

El cociente de verosimilitudes, consiste basicamente en calcular la `verosimilitud del modelo especifico` para los datos. Esta verosimilitud se calcula en dos pasos. 

1. Calcula el `estimador puntual` (via `maxima verosimilitud`) de `\theta_j`, para `j=1,\ldots,J`, con base en los datos $\{x_1,\ldots,x_n\}$. 

Recuerden, de los datos extraemos los datos unicos $\{x_1^*,\ldots,x_U^*\}$ (las frecuencias de los datos unicos no son requeridas en este contexto).

2. Con el `estimador puntual del modelo especifico`, $F_{j}(x|\hat{\theta}_j)$, recuperamos su correspondiente densidad y calculamos la `verosimilitud del modelo` $j$ de la siguiente forma:
$$
lik(j)=\prod_{u=1}^{U}f_j(x_u^*|\hat{\theta}_{j}),
$$
para $j=1,\ldots,J$.

Por cuestiones numericas, es conveniente trabajar con la `log-verosimilitud`,
$$
loglik(j)=\sum_{u=1}^{U} \text{log}f_j(x_u^*|\hat{\theta}_{j}).
$$

**Noten** que la verosimilitud del modelo se calcula sobre un segmento de los mismos datos con los que se estimo el modelo, `datos de entrenamiento`.

## Verosimilitud predictiva

Una variante del `procedimiento basado en verosimilitud` para evaluar la capacidad predictiva del modelo consiste en realizar la siguiente modificacion:
$$
ploglik(j)=\sum_{u=1}^{U} \text{log}f_j(x_u^t|\hat{\theta}_{j}),
$$
donde $\{x_u^t\}_{u=1}^{U}$ corresponden a los $U$ valores unicos de los `datos de prueba/test`. 

**Noten** que la evaluacion de las densidades en los datos de prueba brinda una cuantificacion de que tanta masa de probabilidad el modelo predictivo asigna a los valores que realmente ocurrieron (pero que no fueron empleados en el proceso de estimacion).

# Ejercicios

* Realizar el `Proceso de aprendizaje I` completando la estimacion de las distribuciones faltantes.

* Realizar el `Proceso de aprendizaje II`

# Lectura complementaria

* Kleiber & Kotz, *Statistical Size Distribution in Economics and Actuarial Science*. 

  - Capitulo 3 (distribucion Pareto).

  - Capitulo 4 (distribucion lognormal).

  - Capitulo 5 (distribucion gamma generalizada).

  - Capitulo 6 (distribucion beta generalizada).

* Embrechts et al. *Modelling Extremal Events for Insurance and Finance*. 

  - Capitulos 3 y 6 (distribucion Pareto generalizada).